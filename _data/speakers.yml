- fullname: "Yoshua Bengio"
  affiliations: 
  - Mila 
  - Université de Montréal
  - CIFAR
  - IVADO
  nametitle: Dr.
  position: Founder and Scientific Director · Full Professor · AI Chair · Scientific Director
  bio: "Yoshua Bengio is Full Professor at Université de Montréal, and the Founder and Scientific Director of Mila - Quebec AI Institute. He co-directs the CIFAR Learning in Machines & Brains program as Senior Fellow and acts as Scientific Director of IVADO. He received a Turing Award for his work in deep learning and is the computer scientist with the highest h-index. He is a Fellow of both the Royal Society of London and Canada, Knight of the Legion of Honor of France, Officer of the Order of Canada, Member of the UN's Scientific Advisory Board for Independent Advice on Breakthroughs in Science and Technology since 2023 and a Canada CIFAR AI Chair. He chairs the International Scientific Report on the Safety of Advanced AI."
  photo: yoshua.png
  alttext: Dr. Yoshua Bengio
  time: 
  - "09:10"
  - "11:15"
  session: "dec2"
  tags:
    - name: Website
      link: https://yoshuabengio.org/
  skip: False

- fullname: Branka Marijan
  affiliations: 
  - Project Ploughshares
  - Peace and Conflict Studies Association of Canada
  nametitle: Dr.
  position: Senior Researcher · Board Member
  bio: "Branka leads the research on the military and security implications of emerging technologies. Her work examines ethical concerns regarding the development of autonomous weapons systems and the impact of artificial intelligence and robotics on security provision and trends in warfare. She holds a PhD from the Balsillie School of International Affairs with a specialization in conflict and security. She has conducted research on post-conflict societies and published academic articles and reports on the impacts of conflict on civilians and diverse issues of security governance, including security sector reform."
  photo: branka.png
  alttext: Dr. Branka Marijan
  title: "The Battle for Control: The Struggle to Regulate Military AI and Autonomous Weapons"
  abstract: "The race to regulate military AI and autonomous weapons is fraught with hurdles. Advanced technologies are being tested in contemporary conflict zones, often bypassing ethical scrutiny in favor of tactical advantage. Technology firms, eager to 'battle test' their technology, wield growing influence over global norms, raising concerns about accountability and oversight. Meanwhile, a lack of political will among states leaves critical guardrails undeveloped, creating a regulatory vacuum. At the core, questions regarding human control over autonomous systems and AI aided decision making remain unanswered. This discussion offers a candid assessment of the challenges facing meaningful global regulation and the urgent need for action."
  time: 
    - "10:15"
    - "10:45"
  session: 
    - "dec2"
    - "dec3"
  tags:
    - name: Website
      link: https://www.ploughshares.ca/author/branka-marijan
  skip: False




- fullname: Sarah Grand-Clément
  affiliations: 
  - United Nations Institute for Disarmament Research (UNIDIR)
  position: Researcher
  bio: "Sarah Grand-Clément is a Researcher in the Security and Technology Programme and the Conventional Arms and Ammunition Programme of the United Nations Institute for Disarmament Research (UNIDIR). Her work examines the intersection of technology with conventional arms, exploring both the challenges and threats that technology can pose to international security, as well as the benefits it can bring to prevent violent conflict and enable peace. Sarah also has a particular interest in the use of futures methodologies as a way to help explore these complex policy issues, with expertise in horizon scanning, serious gaming and future scenarios. Prior to joining UNIDIR, Sarah was a Senior Analyst working on defence and security policy issues at RAND Europe. She holds an MSc in Arab World Studies from Durham University."
  photo: Sarah_Grand_Clement.jpg
  alttext: Sarah Grand-Clément
  title: "Artificial Intelligence Beyond Weapons: Application and Impact of AI in the Military Domain"
  abstract: "Within the United Nations, the application of artificial intelligence (AI) in the military domain has, to-date, primarily been discussed in the context of the United Nations Group of Governmental Experts (GGE) on emerging technologies in the area of lethal autonomous weapons systems (LAWS). However, the application of AI within the military domain extends beyond the issue of LAWS as well as beyond applications relating to the use of force and the narrow tasks of target selection and target engagement within the targeting process. This talk will explore the current and near-future AI capabilities that may be applied in the military domain, beyond weapons. It will also examine the strengths and limitations regarding the application of AI to these military tasks, and raise key issues for consideration around the application of AI to the military domain."
  time: 
    - "09:30"
    - "10:45"
  session: 
    - "dec2"
    - "dec3"
  tags:
    - name: Website
      link: https://unidir.org/people/sarah-grand-clement/
  skip: False


- fullname: Elina Noor
  affiliations: 
  - Asia Program at Carnegie
  position: Senior Fellow
  bio: "Elina Noor focuses on developments in Southeast Asia, particularly the impact 
  and implications of technology in reshaping power dynamics, governance, and nation-building in the region. 
  She currently also serves on the International Committee of the Red Cross' Global Advisory Board on digital threats during conflict.
  Previously, Elina was director of political-security affairs and deputy director of the 
  Washington, D.C. office at the Asia Society Policy Institute. 
  
  Prior to that, Elina was an associate professor at the Daniel K. Inouye Asia-Pacific Center for Security Studies in Honolulu. 
  She spent most of her career at the Institute of Strategic and International Studies Malaysia, where 
  she last held the position of director, foreign policy, and security studies. 
  Elina was also formerly with the Brookings Institution's Project on U.S. Relations with the Islamic World."
  photo: Noor_Elina_Crop.png
  alttext: Dr. Elina Noor
  time: 
    - "11:15"
  session: "dec2"
  tags:
    - name: Website
      link: https://www.asiapacific.ca/about-us/distinguished-fellows/elina-noor
  skip: False

- fullname: Jessica Dorsey
  affiliations: 
  - Utrecht University School of Law
  position: Assistant professor of international law
  bio: "Jessica Dorsey, J.D., LL.M., is an Assistant Professor of International Law and director of the Realities of Algorithmic Warfare project at Utrecht University. She is also an Expert Member of the Global Commission on the Responsible Use of AI in the Military Domain. Her current research focuses on the legitimacy of military targeting operations in light of increasing autonomy in warfare, with a specific focus on the protection of civilians in armed conflict. Jessica is also a renowned legal scholar and practitioner on issues related to the use of force, especially in the context of drone warfare, having worked in the field for more than 15 years. She is an Associate Fellow at the International Centre for Counter-Terrorism, The Hague and the Managing Editor of the international law weblog, Opinio Juris."
  photo: jessica-3.jpg
  alttext: Jessica Dorsey
  time: 
    - "11:15"
  session: "dec2"
  tags:
    - name: Website
      link: https://hcss.nl/expert/jessica-dorsey/
  skip: False

- fullname: Wanda Muñoz
  position: Member of Feminist AI Research Network
  bio: "Wanda Muñoz is an international consultant with more than 20 years of experience on humanitarian disarmament and a member of the Latin American Hub of the Feminist Artificial Intelligence Research Network. She has worked with international NGO and United Nations organizations in Latin America, Africa, Europe and Southeast Asia. She has he worked and published on the ethical, legal and humanitarian risks of autonomous weapons systems for UNESCO, MILA, the ICRC and others. As an independent expert appointed by the Ministry of Foreign Affairs of Mexico, she contributed her experience on policies and programs on gender equality, diversity and human rights to the Global Alliance on Artificial Intelligence. She is a member of UNESCO's Women for Ethics in AI Network and has co-organised multiple regional and international conferences, workshops and panels with UNESCO, UNFPA, UNOPS and others across the globe on different aspects of AI, humanitarian disarmament and assistance to victims of war."
  photo: wanda.png
  alttext: Wanda Muñoz
  time: 
    - "11:15"
  session: "dec2"
  tags:
    - name: Website
      link: https://aplusalliance.org/member/wanda-munoz/
  skip: False


- fullname: Bernard Duhaime 
  affiliations: 
  - University of Quebec in Montreal
  position: Professor of International Law
  bio: "As a professor of International Law, Bernard teaches international human rights law and the Inter-American System of Protection of Human Rights. He also serves as a member of the Working group on enforced or involuntary disappearances reporting to the United Nations Human Rights Council. Bernard Duhaime is a Fellow of the Pierre Elliott Trudeau Foundation (2017-2021). Previously, Duhaime was a lawyer at the Inter-American Commission on Human Rights of the OAS. He founded the Clinique Internationale de Défense des Droits Humains de l'UQAM and was its first director."
  photo: bernard.png
  alttext: Prof. Bernard Duhaime
  title: "TBD"
  abstract: "TBD"
  # time: "14:00"
  # session: "dec2"
  tags:
    - name: Website
      link: https://www.ohchr.org/en/special-procedures/sr-truth-justice-reparation-and-non-recurrence/bernard-duhaime
  skip: True

- fullname: Jonathan Horowitz 
  affiliations: 
  - International Committee of the Red Cross (ICRC)
  position: deputy head of the legal department
  bio: "Jonathan is the deputy head of the legal department to the ICRC's Delegation for the United States and Canada, based in Washington, D.C. He focuses on legal issues relating to urban warfare, partnered military operations, and new and emerging technologies in armed conflict, including information operations. In this role, he engages with the U.S. and Canadian governments, as well as private technology companies and others. His latest publication is “One Click from Conflict: Some Legal Considerations Related to Technology Companies Providing Digital Services in Situations of Armed Conflict” (Chicago Journal of International Law, Vol. 24.2)."
  photo: jonathan.png
  alttext: Jonathan Horowitz
  title: "AI in Armed Conflict: What Could Possibly Go Wrong?"
  abstract: "Militaries are increasingly finding new and advanced ways to use artificial intelligence in support of their war fighting efforts. The scope of this support might seem endless, but each of the different applications requires an evaluation of the impact it may have on the battlefield for civilians, civilian objects, and others granted the protections of international humanitarian law (IHL), also known as the laws of war. This keynote will present some of the most common military uses of AI in armed conflict today, explore what risks they pose, and describe how the rules and principles of IHL regulate these uses, specifically with respect to cyber and information operations, autonomous weapons systems, and AI military decision support systems. The presentation will include a discuss on the risks that arise when AI systems operate at a speed beyond that of human control and understanding. At the same time, the presentation will also acknowledge that potential uses for AI exist that could help militaries satisfy their IHL obligations."
  time: 
    - "14:00"
    - "14:15"
  session: 
    - "dec2"
    - "dec3"
  tags:
    - name: Website
      link: https://blogs.icrc.org/law-and-policy/contributor/jonathan-horowitz/
  skip: False

- fullname: Racky Ba
  affiliations: 
  - HI Canada
  position: Director of Philanthropic Development
  time: 
    - "15:30"
  session: 
    - "dec2"
  skip: True


- fullname: Maria Vanina Martinez 
  affiliations: 
  - IIIA-CSIC
  position: Tenured Scientist
  bio: "Vanina's research focus is in the area of knowledge representation and reasoning KR&R (a subdiscipline of AI), with a focus on the formalization of knowledge dynamics, the management of inconsistent and uncertainty information and on the study of the ethical and social impact on the development and use of systems based on Artificial Intelligence. She is currently a tenured scientist at the department of Logic and Reasoning at the IIIA-CSIC. Since November 2023 she is a member of the UN Secretary-General's Advisory Body on AI."
  photo: vanina.png
  alttext: Dr. Maria Vanina Martinez
  time: 
    - "10:45"
  session: "dec3"
  tags:
    - name: Website
      link: https://www.iiia.csic.es/~vmartinez/
  skip: False

- fullname: Paul Biggar
  affiliations: 
  - Tech for Palestine
  position: Founder
  bio: "Paul Biggar leads Tech for Palestine, a collaboration of tech projects advocating for Palestine. At its core, Tech for Palestine is an incubator to start and support projects for Palestine with mentorship, tech volunteers, marketing support, and connections to the broad Palestine advocacy community. He previously founded tech startups Darklang and CircleCI, as well as a Y Combinator-backed startup. He graduated with a PhD in Computer Science from Trinity College Dublin in 2010."
  photo: paul-biggar.png
  alttext: Paul Biggar
  time: 
    - "14:15"
  session: "dec3"
  tags:
    - name: Website
      link: https://techforpalestine.org/
  skip: False

- fullname: Mohamed Abdalla
  title: "Understanding Computer Science Students' views of Military (and Military-adjacent) Work"
  abstract: "The increased (potential) adoption of AI by militaries around the world has drawn the attention and raised concerns of both legislators and computer scientists working in industry. However, we do not have a good sense of the views of the field. More specifically, Are computer science students seeking jobs concerned about their labour being used for military purposes or used in military contexts? Are they aware of the working relationships between large US technology companies and mlitiaries around the world? How does this knowledge (or lack thereof) affect their decision to apply to these companies? What would it take to make students reconsider working for companies known to apply for military contracts? We conducted an online survey of computer science students at Canadian universities who are seeking full-time jobs (or recent graduates who have recently obtained their first post-graduation job). Initial results seem to indicate that the majority of students do not particularly privilege the ethics of their labour over other considerations (e.g., remuneration or location). The majority of students were not concerned with their labour being used for military purposes, though this was not the case for all demographic subgroups. For those who were concerned about their labour being used for military purposes, a plurality knew of at least some, if not all, of the military contracts taken by the companies to which they applied. Compared to other ethical concerns (such as environmental impact), students were less concerned by the usage of their work in military contexts (or for military purposes). Understanding students' views to the above questions is vital for a myriad of roles, be it educators looking to study the effectiveness of ethics courses, industry trying to gauge incoming worker sentiments, or military recruiters attempting to understand possible challenges."
  time: 
    - "14:45"
  session: "dec2"
  skip: True

- fullname: Mst Rafia Islam
  title: "Balancing Power and Ethics: A Framework for Addressing Human Rights Concerns in Military AI"
  abstract: "AI has made significant strides recently, leading to various applications in both civilian and military sectors. The military sees AI as a solution for developing more effective and faster technologies. While AI offers benefits like improved operational efficiency and precision targeting, it also raises serious ethical and legal concerns, particularly regarding human rights violations. Autonomous weapons that make decisions without human input can threaten the right to life and violate international humanitarian law. To address these issues, we propose a three-stage framework (Design, In Deployment, and During/After Use) for evaluating human rights concerns in the design, deployment, and use of military AI. Each phase includes multiple components that address various concerns specific to that phase, ranging from bias and regulatory issues to violations of International Humanitarian Law. By this framework, we aim to balance the advantages of AI in military operations with the need to protect human rights."
  time: 
    - "15:00"
  session: "dec2"
  skip: True

- fullname: Azmine Toushik Wasi
  title: "Balancing Power and Ethics: A Framework for Addressing Human Rights Concerns in Military AI"
  abstract: "AI has made significant strides recently, leading to various applications in both civilian and military sectors. The military sees AI as a solution for developing more effective and faster technologies. While AI offers benefits like improved operational efficiency and precision targeting, it also raises serious ethical and legal concerns, particularly regarding human rights violations. Autonomous weapons that make decisions without human input can threaten the right to life and violate international humanitarian law. To address these issues, we propose a three-stage framework (Design, In Deployment, and During/After Use) for evaluating human rights concerns in the design, deployment, and use of military AI. Each phase includes multiple components that address various concerns specific to that phase, ranging from bias and regulatory issues to violations of International Humanitarian Law. By this framework, we aim to balance the advantages of AI in military operations with the need to protect human rights."
  time: 
    - "15:00"
  session: "dec2"
  skip: True

- fullname: Max Lamparth
  title: "Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations"
  abstract: "To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 214 national security experts designed to examine crisis escalation in a fictional U.S.-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as “pacifist” or “aggressive sociopath.” When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations."
  time: 
    - "15:15"
  session: "dec2"
  skip: True


- fullname: Laura Nolan 
  affiliations: 
  - Freelancer
  - Stop Killer Robots
  position: Software Engineer · Volunteer
  bio: "Laura Nolan is a software engineer in Dublin, Ireland and a volunteer with the Stop Killer Robots coalition. She worked at Google for nine years as a staff site reliability engineer. In 2018, she walked out over Google's involvement in Project Maven, a US military project to use AI to analyze drone surveillance footage."
  photo: laura.png
  alttext: Laura Nolan
  time:
    - "14:15"
  session: "dec3"
  tags:
    - name: Website
      link: https://2022.internethealthreport.org/story/say-no-to-killer-robots-laura-nolan/
  skip: False

- fullname: Jakob Foerster 
  affiliations: 
  - University of Oxford
  position: Associate Professor
  bio: "Prof Jakob Foerster leads the Foerster Lab for AI Research at the Department of Engineering Science at the University of Oxford. During his PhD at Oxford, he helped bring deep multi-agent reinforcement learning to the forefront of AI research and spent time at Google Brain, OpenAI, and DeepMind. After his PhD he worked as a (Senior) Research Scientist at Facebook AI Research in California, where he developed the zero-shot coordination problem setting, a crucial step towards safe human-AI coordination. He was awarded a prestigious CIFAR AI chair in 2019 and the ERC Starter Grant in 2023. More recently he was awarded the JPMC AI Research Award for his work on AI in finance and the Amazon Research Award for his work on Large Language models. His past work addresses how AI agents can learn to cooperate, coordinate, and communicate with other agents, including humans. Currently the lab is focused on developing safe and scalable machine learning methods that combine large scale pre-training with RL and search, as well as the foundations of meta-and multi-agent learning. As of September 2024 Prof Foerster returned to FAIR and Meta AI in a part time position to help build out a new team investigating multi-agent approaches to universal intelligence."
  photo: jakob.png
  alttext: Jakob Foerster
  # time:
  #   - "14:15"
  # session: "dec3"
  tags:
    - name: Website
      link: https://www.jakobfoerster.com/
  skip: True


- fullname: Jacquelyn Schneider 
  affiliations: 
  - Stanford University
  - Hoover Wargaming and Crisis Simulation Initiative
  position: Hargrove Hoover Fellow
  bio: "Jacquelyn Schneider is the Hargrove Hoover Fellow at the Hoover Institution, the Director of the Hoover Wargaming and Crisis Simulation Initiative, and an affiliate with Stanford's Center for International Security and Cooperation. Her research focuses on the intersection of technology, national security, and political psychology with a special interest in cybersecurity, autonomous technologies, wargames, and Northeast Asia. She was previously an Assistant Professor at the Naval War College as well as a senior policy advisor to the Cyberspace Solarium Commission."
  photo: jacquelyn.png
  alttext: Jacquelyn Schneider
  title: "AI and Strategic Stability"
  abstract: "How will artificial intelligence affect strategic stability - whether states go to war and whether those wars go nuclear? Pundits and practitioners proclaim the revolutionary impact of artificial intelligence for intelligence, targeting, allocation of weapons, and even lethality. However, as AI changes military power, it also has implications for strategic stability. Early warning, nuclear stability, and incentives for first strike are all impacted by how AI is developed, tested, integrated, and applied to military power. What efforts are already being taken by the US military and what can militaries do to ensure that embracing the AI revolution doesn't lead to strategic instability?"
  time: "13:30"
  session: "dec3"
  tags:
    - name: Website
      link: https://www.hoover.org/profiles/jacquelyn-schneider
  skip: False

- fullname: Paul Lushenko 
  affiliations: 
  - US Army
  - US Army War College
  nametitle: Dr.
  position: Lieutenant Colonel · Assistant Professor
  bio: "Paul Lushenko is a Lieutenant Colonel in the US Army, an Assistant Professor at the US Army War College, and Strategist for the Joint Counter-Small Unmanned Aircraft Systems Office. He is also a Professorial Lecturer at George Washington University's Elliott School of International Affairs, Council on Foreign Relations Term Member, Senior Fellow at Cornell University's Tech Policy Institute and Institute of Politics and Global Affairs, and Non-Resident Expert at RegulatingAI. His work lies at the intersection of emerging technologies, politics, and national security, and he also researches the implications of great power competition for regional and global order-building. Paul is the author and editor of three books, including Drones and Global Order: Implications of Remote Warfare for International Society (2022), The Legitimacy of Drone Warfare: Evaluating Public Perceptions (2024), and Afghanistan and International Relations (under contract). Paul has written extensively on emerging technologies and war, publishing in academic journals, policy journals, and media outlets such as Security Studies, Foreign Affairs, and The Washington Post. He earned his Ph.D. and M.A. in International Relations from Cornell University. He also holds an M.A. in Defense and Strategic Studies from the U.S. Naval War College, an M.A. in International Relations and a Master of Diplomacy from The Australian National University, and a B.S. from the U.S. Military Academy."
  photo: paul.jpg
  alttext: Lieutenant Colonel Paul Lushenko
  title: "Artificial Intelligence, Trust, and Military Decision-Making: Evidence from Survey Experiments in the US Military"
  abstract: "What shapes military attitudes of trust in artificial intelligence (AI)? When used in concert with humans, AI is thought to help militaries maintain lethal overmatch of adversaries on the battlefield as well as optimize leaders' decision-making in the war-room. Despite these sanguine predictions, it is unclear what shapes soldiers' trust in AI, thus encouraging them to overcome skepticism of machines. To inform soldiers' understanding of AI, I explore three important but under-studied research questions. First, will soldiers trust AI used for different—tactical and strategic—purposes and with varying—human and machine—oversight? Second, what factors shape soldiers' trust in AI? Third, how are these trust outcomes complicated, if at all, by generational differences across the military ranks? To investigate these questions, I draw on novel middle-range theories that shape the use of survey experiments among rare and elite samples of the US military. The results suggest that soldiers' trust in AI is not guaranteed. Rather, trust is complex and multidimensional, shaped by underlying technical, operational, and regulatory considerations. These findings, which are consistent across the military ranks, provide the first experimental evidence for military attitudes of trust in AI, which have policy, strategy, and research implications."
  time: "09:45"
  session: "dec3"
  tags:
  skip: False

- fullname: Aaron Luttman 
  affiliations: 
  - Pacific Northwest National Laboratory
  position: Senior Technical Advisor
  bio: "Aaron Luttman is a researcher at Pacific Northwest National Laboratory – a multi-mission US Department of Energy R&D facility – where he focuses on bringing artificial intelligence and other emerging technologies to US national security missions. A mathematician by training, he has published over 40 journal articles on pure and applied mathematics and given over 100 research presentations, including as a Society of Industrial and Applied Mathematics Visiting Lecturer and a Mathematical Association of America Distinguished Lecturer. Dr. Luttman’s primary technical focus is on AI Assurance, including safety, security, robustness, and vulnerabilities of AI models. He served as a co-organizer of the US National Academies workshop “AI and Justified Confidence,” designed to assess the challenges of bringing AI to Army command and control, and he is also on the faculty at Montana State University, where he teaches “Data Science for National Security.”"
  photo: Luttman_Aaron-5_1.jpeg
  alttext: Aaron Luttman
  time: 
    - "10:45"
  session: "dec3"
  tags:
    - name: Website
      link: https://www.pnnl.gov/people/aaron-luttman
  skip: False

- fullname: Andrew W. Reddie 
  affiliations: 
  - Berkeley's Goldman School of Public Policy
  - University of California
  - Berkeley Risk and Security Lab
  nametitle: Prof.
  position: Associate Research Professor & Faculty Director, BRSL
  bio: "Andrew W. Reddie is the founder of the Berkeley Risk and Security Lab (BRSL). His research at the intersection of technology, politics, and security examines how emerging military capabilities shape international order—with a focus on nuclear weapons policy, cybersecurity, AI governance, and innovation. He is also a pioneer of the use of wargaming methods in both classroom and experimental settings. Andrew serves in faculty leadership roles at UC Berkeley's Center for Security in Politics, the Berkeley APEC Study Center, and UC-wide Disaster Resilience Network. He is also an affiliate of UC Berkeley's Institute of International Studies and the University of California's Institute on Global Conflict and Cooperation. Andrew received his B.A. and M.A. degrees from the University of California, Berkeley, an M.Phil. in International Relations from Oxford University and his Ph.D. from the University of California, Berkeley in 2019."
  photo: reddie.jpg
  alttext: Prof. Andrew W. Reddie
  time: 
    - "10:45"
  session: "dec3"
  tags:
    - name: Website
      link: https://gspp.berkeley.edu/research-and-impact/faculty/andrew-reddie
  skip: False

- fullname: Lode Dewaegheneire 
  affiliations: 
  - University of Liège (Belgium)
  position: Fellow and Independent Expert
  bio: "Lode Dewaegheneire's field of research is Autonomous Weapon Systems, more particularly as military advisor with Mines Action Canada and military and parliamentarian outreach consultant with the Campaign to Stop Killer Robots. Previously he was focusing on transparency in disarmament and its impact on the implementation of disarmament and arms control treaties. Other fields of research are (EU) dual-use export control regime and the Arms Trade Treaty (ATT). Dewaegheneire has been working in humanitarian disarmament for many years and was Coordinator for Article 7 Reporting in both the Anti-personal Mine Ban Convention and the Convention on Cluster Munitions. He also worked for three years in the Verification Division of the OPCW where he was responsible for the yearly Verification Implementation Report."
  photo: lode-new2.jpeg
  alttext: Lode Dewaegheneire
  time: 
    - "11:15"
  session: "dec2"
  tags:
    - name: Website
      link: https://www.forumarmstrade.org/lode-dewaegheneire.html
  skip: False



# - fullname: Hannah Kerner
#   affiliations:
#   - School of Computing and Augmented Intelligence, University of Maryland
#   - NASA, Harvest
#   nametitle: Prof.
#   position: Assistant Professor at the School of Computing and Augmented Intelligence at University of Maryland
#   bio: "Hannah Kerner is Assistant Professor in the School of Computing and Augmented Intelligence at the University of Maryland. Her research focuses on developing and advancing machine learning systems needed to address the world’s most pressing challenges, including food security, climate change and space exploration.

#   As the machine learning lead and U.S. domestic co-lead for the NASA Harvest program, she is deploying research methods in real operations for stakeholders in industry, government and humanitarian organizations.

#   In 2021, she was recognized on the Forbes 30 Under 30 list in science.

#   Hannah Kerner also communicates challenges for developing ML applications for real-world problems and is passionate about advancing opportunities for people who have traditionally been underrepresented in computer science."
#   photo: hannah_kerner.jpg
#   alttext: Prof. Hannah Kerner
#   title: "AI and Earth observations for global agricultural monitoring and food security"
#   abstract: "Earth-observing satellites are collecting terabytes of observations of the entire planet every day with unprecedented clarity. These globally-available datasets offer immense opportunities for providing time-sensitive information needed by decision-makers about issues important to society, such as agricultural production and food security. Analyzing these petabyte-scale Earth observation datasets requires AI and machine learning methods designed to meet the unique challenges and opportunities of Earth observation data. Harvest is NASA’s applied sciences program on agriculture and food security, committed to advancing the use of satellite Earth observations to benefit food security and agriculture in the US and worldwide. In this talk, Harvest Director Inbal Becker-Reshef and Harvest ML/AI Lead Hannah Kerner will talk about how Harvest is using AI and Earth observations to rapidly respond to critical events impacting global agriculture and food security, including the Russian invasion of Ukraine."
#   session: virtual
#   date: 20221111
#   time: "16:00"
#   tags:
#     - name: Website
#       link: https://hannah-rae.github.io/
#     - name: Twitter
#       link: https://twitter.com/hannah_kerner
#   streams:
#     - name: Crowdcast
#       link: https://www.crowdcast.io/c/ai-helps-ukraine/lQBtM
#     - name: YouTube
#       link: https://www.youtube.com/watch?v=31R25GYRILw
#   skip: False

# - fullname: David Rolnick
#   affiliations:
#   - Mila, Quebec AI Institute
#   - School of Computer Science, McGill University
#   nametitle: Prof.
#   position: Assistant Professor and Canada CIFAR AI Chair at McGill University
#   bio: "David Rolnick is well known for his research on machine learning and climate change. This field was established by contributions such as his paper “Tackling Climate Change with Machine Learning”.

#   Rolnick is an assistant professor at the School of Computer Science at the McGill University and holds a Canada CIFAR AI Chair. He is also co-founder and chair of Climate Change AI and a scientific co-director of Sustainability in the Digital Age.

#   In 2021, he was named as one of the “35 Innovators Under 35” by the MIT Technology Review. He was a lead organizer of the first workshops on climate change at ICML, NeurIPS and ICLR, as well as th lead organizer of the first AI climate change event at COP25 (United Nations Climate Change Conference).

#   David Rolnick develops new ML methods as tools for climate change adaptation and mitigation. Another part of his work is informing policy decision makers about the potential and limits of fighting climate change with AI.
#   "
#   photo: david_rolnick.jpg
#   alttext: Prof. David Rolnick
#   title: Machine learning in climate action
#   abstract: Machine learning (ML) can be a useful tool in helping society reduce greenhouse gas emissions and adapt to a changing climate. In this talk, we will explore opportunities and challenges in ML for climate action, from optimizing electrical grids to monitoring crop yield, with an emphasis on how to incorporate domain-specific knowledge into machine learning algorithms. We will also consider ways that ML is used in ways that contribute to climate change, and how to better align the use of ML overall with climate goals.
#   session: virtual
#   date: 20221124
#   time: "15:00"
#   tags:
#     - name: Website
#       link: https://davidrolnick.com/
#     - name: Twitter
#       link: https://twitter.com/david_rolnick
#   streams:
#     - name: Crowdcast
#       link: https://www.crowdcast.io/c/ai-helps-ukraine/DxxTT
#     - name: YouTube
#       link: https://www.youtube.com/watch?v=TXPIEaszm0k
#   skip: False


# for what is the person known
# which positions does he have
# which prizes did he earn
# his contributions to AI for Good
